<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Siri in the Uncanny Valley</title>
    <style media="screen">
      p {
        text-indent: 50px;
      }
    </style>
  </head>
  <body>
    <h2>The Godfather of Artificial Intelligence</h2><br>
    <p>
      Alan Turing is probably the most famous Cryptographer in history. He is definitely the only cryptographer to lead a team of English logicians under wartime conditions to break the cipher behind Germany’s original “unbreakable code” during world War II, the Enigma code. As a result, he was also directly responsible for the sinking of a lot of Nazi U-boats, all of which is the type of thing that makes a person pretty famous. Therefore, when speaking of the all-too-short life of Alan Turing, it is usually only mentioned second that he is the godfather of both the computer and artificial intelligence.
    </p><br>
    <p>
      He penned some of the most important early papers in these brand new (at the time) fields, literally establishing many of the building blocks in Computer Science. He designed the world’s first stored-program computer, or what we think of today as simply “a computer”, as well as writing a very early paper describing a “Turing Machine”, a paper upon which John Von Neumann would use as the basis to invent the architecture behind microchips. He even beat Von Neumann to the stored-program computer, who had only managed to eke out partial design of the same thing the year before. Turing was also one of the greatest mathematicians in human history, proving without a shadow of a doubt that some people are simply genius.
    </p><br>
    <p>
      On the subject of computers, Turing’s most famous papers is called Computing Machinery and Intelligence [1], a brilliant piece written about computer learning. This paper  was the first to ask the question “could a computer think”, and Turing came to that, in theory, yes they could. In “Computer Machinery”, Alan Turing also changed the face of AI even by inventing the The Turing Test, the one and only criteria used today universally to judge whether a program/application/AI can pass as human. Explaining that most definitions of “think” are entirely too ephemeral to judge with repeatable metrics, he devised the test as a standardized method for deciding the quality of AI. Today, the Turing Test is the single most-used method for judging whether a program/application/AI can “pass as human”.
    </p><br>
    <p>
      Turing envisioned his test, called The Imitation Game (also the name of the great movie about his life[2]), a game which takes place with two humans and one artificial intelligence computer, all three of which are in separate rooms and can only communicating via written communication. One moderator could ask the other two as many questions as they liked, until this moderator thought they knew which one respondee was the computer. If the computer lasted more than five minutes without the moderator guessing their identity, or if they guess wrong, the computer passes the test.
    </p><br>
    <p>
      These days, the Turing Test has mostly dropped the game aspect, and instead stands as shorthand for a computer which “can pass for human to an average person” or, more honestly “passes for human to me”. It is not uncommon for the media to use Turing Test in articles[3] about artificial intelligence or machine learning, if nothing else because it’s a criteria that makes sense to their readers. If it sounds like a human enough that it fools you, it passes the test.
    </p><br>
    <p>
      Alan Turing set a high bar for artificial intelligence that, nonetheless, seemed very achievable to him. In fact, he thought that the main roadblock to achieving true AI was processing power, a proposition we will come to see is far from complete. He thought that by the twenty-first century we would have machines that mimic human thought patterns and language well enough to fool us into thinking they were humans. Instead we have Siri, which does not exactly fulfill Turing’s dreams of artificial intelligence.
    </p><br>
    <br><h2>Siri is not helpful in a snowstorm</h2><br>
    <p>
      Alan Turing predicted we would have artificial intelligences which could convince us for five minutes straight that they were human. Siri, on the other hand, struggles to open her digital mouth for more than 5 seconds without giving away how non-human and robotic she is, which I learned first-hand while on a road trip.
    </p><br>
    <p>
      I was in the middle-of-nowhere Pennsylvania in my hatchback Honda Civic, driving New York to Chicago back from a Thanksgiving holiday weekend, on the I-80 which had been cleared of snow so recently you could see the treadmarks from the plow. I was driving alone, and had the foresight to plan my entertainment in great detail, but not to avoid the storm. I was listening to podcasts (How DId this Get Made and Doug Loves Movies), an audiobook (Parallel Worlds by Michio Kaku), and music, and I wanted to a way to toggle between all of these while minimizing my time spent looking at the screen. Naturally, Siri came to mind, and as I had the new iPhone this seemed like a natural solution. After all, wasn’t Siri some kind of tool that could control apps and do search? That’s what I had heard, at least.
    </p><br>
    <p>
      I then went through a scenario that might seem familiar to many of you readers. I waited until I would soon need to make my first gas stop, and cued Siri by holding down the big center button. I was encouraged to feel the familiar haptic double-tap that iPhone’s give when they want you to feel comfortably-alerted that you might need to do something.
    </p><br>
    <blockquote>
      &#34;Siri, how far is the closest gas station?&#34;
    </blockquote><br>
    <blockquote>
      &#34;Searching &#39;closest gas station&#39; in Bing.&#34;
    </blockquote><br>
    <p>
      I had triggered a search, something which I (rightly) guessed was a default response when Siri doesn’t understand an explicit command. But no I was stuck with Bing opening a page filled with wikipedia articles about gas stations, and only a fuzzy understanding of how to get back to Siri’s home screen to correct the mistake.
    </p><br>
    <p>
      Also, Bing? Siri uses Bing?
    </p><br>
    <p>
      I was starting to get a bad feeling about this whole thing. I went back to the home screen, holding down the center button again to get int Siri’s home screen.
    </p><br>
    <blockquote>
      &#34;No no, Siri, where is the closest gas station?&#34;
    </blockquote><br>
    <blockquote>
      &#34;Searching, &#39;Closet gasket shove&#39; in Bing.&#34;
    </blockquote><br>
    <p>
      Obviously, I needed an alternative approach, as either Siri could not access Apple’s Map App to search for gas stations while inside it’s home screen, or would not do so for me. Since Apple maps was proprietary, and driving directions must be one of the most important early use cases for Siri’s voice technology, Siri must work with this app, right? “Right”, I told myself optimistically, as I saw my gas gauge dip below the red line.
    </p><br>
    <blockquote>
      &#34;Siri, open gas station in maps, please.&#34;
    </blockquote><br>
    <blockquote>
      &#34;Opening &#39;station maps&#39; in Bing&#39;.&#34;
    </blockquote><br>
    <p>
      I added the please, figuring it couldn’t hurt. After all, when I swore at Siri (admit it, you do it too) she pretended to get miffed with me. If she could get her feelings hurt, I thought, maybe a “please” would help her feel good about herself, so she would try extra hard to solve my gas station issue.
    </p><br>
    <blockquote>
      &#34;Siri, search &#39;gas stations&#39; in Apple Maps&#34;
    </blockquote><br>
    <blockquote>
      &#34;Opening Maps&#34;
    </blockquote><br>
    <p>
      Finally, we were getting somewhere! I got immediately excited, and waited for a safe opportunity to take a peek at the screen. A car blew by me and when I finally saw no one ahead or behind me, took a look. Ugh! Nothing about gas stations of any kind, Siri had just opened Apple Maps to my last search (which was for Dunkin Donuts in the Chicago Loop). Clearly, there was a major failure to communicate between me and Siri.
    </p><br>
    <p>
      This experience, and others, inspired a serious interest in voice technology, and subsequently the deeper question of interaction between humans and machine intelligence. I am a linguist by education and software programmer by trade, so I had some tools at my disposal. But I would discover that there are no easy answers to the fascinating questions raised by people from Alan Turing to Bill Gates.
    </p><br>
    <br><h2>Pop Culture Robots</h2><br>
    <p>
      Pop culture has led us to expect so much when it comes to voice technology, when so little has actually been delivered, leading especially us sci-fi loving nerds with high expectations for talking computers which have never remotely been achieved in the real world.
    </p><br>
    <p>
      When I think of the perfect, human-like, talking robot, I think of Priss from Blade Runner. One of my all-time favorite favorite movies, Blade Runner is a about a group of human-like robots, called replicants, who can climb walls, wax poetic about the nature of death, and even perform a strip-tease with a snake, who escape from slave-labor off-world colony and go hide out on Earth, where they are outlawed. The movie opens:
    </p><br>
    <blockquote>
      &#34;The Nexus 6 replicants were superior in strength and agility, and at least equal in intelligence, to the genetic engineers who created them.&#34;
    </blockquote><br>
    <p>
      The premise is “If a machine looks like us and acts like us, then why what’s the difference between this machine and a human?”, which becomes a very real situation for Harrison Ford as the police-contracted bounty hunter who is supposed to hunt down and kill these robots (a job position called a “Blade Runner”). We see the ostensible “bad guy” robots mourn, laugh, philosophize, and ultimately the leader commits a very unselfish act. This is in contrast to the human characters, who are generally seen as cogs in gigantic bureaucracies, whether the police or the monolithic corporation that builds the newest, Nexus-6 replicants, and have resigned themselves to staying on a dying earth that almost everyone else has abandoned for off-world colonies.
    </p><br>
    <p>
      The movie forces the viewer to ask themselves “What does it mean to be alive?”. It also asks  whether something we create might deserve ‘rights’?”, an issue which Bill Gates brought to the forefront recently, when he proposed that robots should pay taxes. Blade Runner also has an all-time great Harrison Ford performance, Daryl Hannah in her prime, Director Ridley Scott just after the first the first Alien, and a score that was so influential it created it’s own sub-genre of music. If you have not seen this movie, you should.
    </p><br>
    <p>
      Blade Runner presents the viewer with a strange moral dilemma by humanizing the killer robots specifically by giving them attributes and linguistic abilities equal to those of humans. Unfortunately, we didn’t realize how big our suspension of disbelief actually was to accept the Replicants as real. The movie has proven prophetic in terms of many of the technologies it presents, but when it comes to the speaking and conversational abilities of the Nexus-6 it might as well have presented jetpacks to the moon, as we’ve made only a fraction of the progress shown on the screen.
    </p><br>
    <p>
      What we’ve come to find in the real world is that strength and agility are no problem, replacing human physical tasks with robot labor in the real world has largely been conquered, as much factory and farm work has almost entirely replaced human workers with automated machines. Boston Dynamics has released several videos (which quickly went viral), of their swimming, crawling, bat-flapping robots which mimic animal and human movement to perform amazing tasks.
    </p><br>
    <p>
      The problem is when it comes to “intelligence”, the machines are way behind what we expected from our pop culture. The Jetsons and 2001 promised us intellectually stimulating conversations from our robot assistants, and while physical jobs get rapidly automated out of existence by robots that work tirelessly at peak performance (think about all those assembly line jobs, and where they went), when it comes to intelligent machines that can talk or hold a pleasant conversation, the results are not quite so stunning. In fact, we are stuck with the  Siri and Alexa which, while amazing pieces of technological achievement, come nowhere close to the expectations that have been matched and succeeded in the physical realm. Voice technology, much like Marty McFly’s Hoverboard from back to the future, remains struggling in the shallow end of the technological swimming pool.
    </p><br>
    <p>
      Nonetheless, a consumer revolution in voice technology is occurring, and recently the highly-polished Google Home, Amazon Alexa, and Echo Dot have started showing up in households all over the world. Suddenly, voice-based virtual assistants were everywhere, and I am bemused by how little the voice technology has improved. These AI Assistants use incredibly smart Artificial Intelligences, requiring whole warehouses filled with endless rows of servers stacks, all dedicated to interpreting if whether you mumbled “Play Michael Buble” or “Play Michael Bolton” at your Alexa with a forkful of pie in your mouth, a task which they struggle to get consistently right.
    </p><br>
    <p>
      This influx of voice-based consumer products has lead to a myriad of examples like the one I gave above, which reveal a startling truth - voice technology just isn’t very good. Siri, Alexa, and Google Echo make lots of mistakes and it is absolutely expected that the users to pick up the slack. It’s normal if the machine takes a few tries to understand your commands correctly, a user experience consistency problem which would bar most products, either digital or physical, from a  consumer release. Can you imagine a light switch which only flipped on or off two out of three times? Or using a SaaS app like Trello and seeing the perfect feeling drag-and-drop work only one-half the time? It would be very noticeable, and probably gather fairly sharp criticism. But, three of the biggest tech companies in the world, Apple, Amazon, and Google can’t come up with a really good voice assistant that can pass this criteria. This, of course, makes us ask ourselves to think of an interesting question. Why is it so hard to make good voice technology?
    </p><br>
    <p>
      Every iPhone user on the planet has tried Siri, even if only by accident when holding the home button too long and accidentally saying something. Siri might be the most frustrating talking robot out there, as she is great at only occasionally fulfilling your request correctly, always giving the user a sliver of hope that she might choose to do so again in the future. Not likely. Additionally, she has one annoying habit seen quite clearly from my Siri experience explained previously, which is to just search anything it doesn’t understand. Thus, when yelling at your personal device, it will happily research the more creative swear-words you might scream.
    </p><br>
    <p>
      The Amazon Echo, with it’s Alexa AI, and the Google Home now offer competing generalized home-based digital assistant controlled entirely through voice. And they’re both moving millions of units as we speak, and were sold out for months during the holidays. And yet, the two week retention rate for Alexa apps is only 3% [4]. And if you’re like me, if 97% of the apps you download on a platform are so bad that you need to delete them after a week, you’ll eventually stop downloading apps from that platform altogether. While still relatively niche, voice has definitely made it to the mainstream in the technologies we use, warts and all.
    </p><br>
    <p>
      Why is it so hard for Siri and these competitors to become “Turing level”? Is there some fundamental reason why our computers cannot perform a decent conversation?
    </p><br>
    <p>
      The answer is “Yes” but, surprisingly, has very little to do with technology and lots to do with how us human beings perform language. Human language is very complex, much more so than we intuitively realize, and our brains have evolved in many crazy and awesome ways to effortlessly handle the acts of speaking and listening. We don’t realize how sensitive we are to non-human speakers, a linguistic version of the “Uncanny Valley” effect, and I will use this time to explain how this forces voice technology to reach a higher level of quality than other technologies may expect, a level of quality which voice has not yet achieved.
    </p><br>
    <p>
      First, I will explain this “Uncanny Valley” phenomenon, a popularize phrase to describe technology which has reached some level of human-mimicry that it makes people feel uncomfortable. Here, we will show that there is actually quite a narrow range of voice capability from a non-human that we find acceptable, and that Siri really has quite an uphill battle if she is ever to reach that critical level.
    </p><br>
    <p>
      Next, I will dive into language itself, using irregular verbs and strange anachronistic conjugation rules to demonstrate that humans that human mastery of language is an nuanced task, causing Siri and her AI counterparts to struggle. We will see how humans have a deeper, richer understanding of linguistic patterns and informal speech than even we realize, and how this affects communication between us and non-human machines.
    </p><br>
    <p>
      Finally, we’ll look at the future of voice technology, and which products and technologies look like they could leap past some of the hurdles mentioned here. Looking at specific areas of voice that poised for advancement, we’ll see how our software may cross the Uncanny Valley.
    </p><br>
    <br><h2>The Uncanny Valley</h2><br>
    <p>
      A huge problem for the engineers creating voice technology is that we consumers demand realism, and are relentlessly determined to root out a fake. It’s the same part of the brain that refuses to allow us to see CGI faces and think they look real, falling into the Uncanny Valley. As a result, we actually have a linguistic Uncanny Valley, triggered by things that seem close to, but not exactly, human. In this section, I will explain how the Uncanny Valley works, and what may be the cause in the first place.
    </p><br>
    <p>
      The Uncanny Valley, coined by Japanese roboticist Masahiro Mori, is the idea that there is a peak of realism for animated or computer generated characters in TVs and movies. If a character is on the “not-so-realistic” side of this peak, like Barney, C3PO, or any of the Simpsons, viewers will accept the character fairly easily.
    </p><br>
    <p>
      However, if one creates characters that are too realistic, falling to the “hyper-realistic” side of this peak, and into the Uncanny Valley. Hyper-realistic digital characters, such as Tom Hanks character in the The Polar Express or the Rock in Mummy: Curse of the Scorpion King, are rejected strongly by audiences, and it is often talked about as a major negative of any of these movies. We hate uncanny valley characters, they just don’t sit right with us when, oddly, we would be much more comfortable with a much less realistic portrayal. Fans of Malcolm Gladwell will recognize that this is done through the “blink reflex” [5], a snap judgement you make almost instantaneously and without conscious interference, in this case to decide whether it is a person or a fake-simulacra which you are looking at. Let me give you the visual example, and maybe your natural Unnatural Valley instincts will kick in and you’ll see what I’m talking about.
    </p><br>
    <h4>Samurai Jack goes here</h4><br>
    <p>
      That is a picture of Samurai Jack, who is very clearly “Pre-Valley” because, while we understand he is a cartoon representation of a person (a samurai time traveller stuck in a dystopian future, specifically), we are not confused that we are looking at real footage of an actual person like, for example, Tom Cruise. Our base, lizard brain is 100% sure that when we look at Samurai Jack, we are is a fake person which, weirdly enough, allows us to naturally accept him as a character with whom we empathize in the confines of the TV show.
    </p><br>
    <h4>Polar Express goes here</h4><br>
    <p>
      This is Tom Hanks in the Polar Express, a perfect example of a CGI character caught right in the lowest crevasse of the uncanny valley. Here, Tom Hanks has been rendered to look recognizable as A Tom Hanks-like figure, except with plastic skin and dead eyes which seems to be popping from his head. There is just enough unreality at play to make us think that we are looking at some creepy pod-person version of America’s favorite nice guy. It’s off-putting, as our brains can’t stop telling us that there is something seriously wrong with Mr. Hanks.
    </p><br>
    <h4>Cushing goes here</h4><br>
    <p>
      When Peter Cushing showed up in Star Wars: Rogue One in 2017, fans were surprised, to say the least, as Cushing had been dead for 23 years. Using a combination of old footage and digital effects they brought the English actor to the screen for this prequel to the original film. Audiences did not universally love this, and as part of the backlash Disney actually had to promise worried fans that they would not use the image of the recently-deceased Carrie Fisher as princess Leia in any future movies (apparently a promise they’ve already changed their mind about, Disney has announced that Princess Leia will have a digital cameo in Episode IX). However, Cushing’s entrance in the film was a definitive post-valley moment, as most audience members didn’t even realize he was dead.
    </p><br>
    <p>
      The Uncanny Valley is present in language, too, as an audio phenomenon in addition to the visual version described above. We can be easily frustrated by or dislike fake voices when they reach that critical point of “human-ness” to cause discomfort. I personally get annoyed every single time I call a utility and need to speak and listen to their message-tree, having learned long ago to immediately switch into keypad mode, as it makes my skin crawl to speak with a voice that is smart enough to sound normal, but which misinterprets half of my instructions. Like CGI characters, synthetic voices have a range of “realness” which will have the effect of nails on a chalkboard, causing us discomfort and an aversion to the voice we are hearing. If we look at different styles of voice, we can pinpoint roughly where this linguistic Valley lies, keeping in mind that this is subjective to the listener.
    </p><br>
    <p>
      At the “pre-valley” level are every clearly-synthetic voice you’ve ever heard. I think of the nice, electronic-sounding lady on the trains in Chicago, the one who nicely informs me “Next station...Granville” with the grainy, stilted pronunciation that could only come from a voice-emulating program. Nobody could mistake this for a real voice, and so this voice slides easily into our conscious background.
    </p><br>
    <p>
      Right in the middle of the linguistic uncanny valley is Siri. She speaks with an undeniably human-ish voice, understands and responds, and yet could never pass for more than a few seconds as a human voice; or even for a few 1/1000s of a second, since we use our “blink judgement” to decide what is and is not human. Siri acts and responds kind of like a human, she even tries to joke around a bit with certain answers, but I would argue this pushes her even deeper into the valley - these traits definitely don’t make her pass more realistically as a human.
    </p><br>
    <p>
      And finally, we don’t have any real world examples of Post-Valley voice technology, as it simply does not exist yet (hence, the reason for writing this book). But, if we were to look at examples from pop culture, Jarvis from Iron Man or Priss/Roy from Blade Runner would be perfect examples. These are “machines” which can interact with speakers exactly like a human, answering questions, making moral judgements, and even demonstrating emotions. Were these characters in the real world, they would pass the Turing test and fool most, if not all, people they met into thinking that they were also a human being.
    </p><br>
    <p>
      The Uncanny Valley is not random, it’s a very important survival effect that we, and likely many species, developed as an evolutionary trait to help protect us from danger. It’s just not totally clear, at least not from the outset, what exactly this is protecting us so it makes sense to dive a little into our way-far-back history to see how we might have evolved such a strong reaction to anything entering the Uncanny Valley.
    </p><br>
    <br><h2>Voice Predators</h2><br>
    <p>
      I have an evolutionary biology streak, though I’m not always proud. Evolutionary Biology is one of the funnest sciences, but it often leaves one wildly speculating as to the source of genetic traits with no way to prove your theories unless you literally have a time machine (mine’s a Tardis) to take your GoPro back 300,000 years to see which specific creatures were hunting down our ancestors to snack on them like cheetos.
    </p><br>
    <p>
      Nonetheless, our aversion to human-mimicry is definitely adaptive. At some point in our evolutionary history, a threat was killing our would-be ancestors who could not pick out an almost-human voice from a real human voice. Based on examples from the animal kingdom today, this was probably some kind of a mimicking predator - one who could sound enough like a human to lure unsuspecting people to a grim fate.
    </p><br>
    <p>
      Aggressive mimicry is a common predatory offensive tactic found in the animal kingdom, where a predator will use strategies that make it appear harmless or attractive to it’s prey so it can get close enough to successfully attack. A common tactic is to mimic the opposite sex of the prey, using sight, sound, or scent, called “sexual-signal mimicry”. One example is the firefly, a species whose airborne males signal who flash their lights to attract female counterparts on the ground, who respond with their own lights. Yet, sometimes another bio-luminescent species will flash a response to a male’s signal, an adaptive attack strategy to lure the male firefly close enough to grab and eat. Other times, a predator might mimic a prey’s food, looking like a nice snack right up until a final surprising moment for the victim; the tongue of the Alligator Snapping Turtle is bright red, and when it wriggles around it looks a lot like a worm to fish that may be swimming by, often a deadly mistake for the fish.
    </p><br>
    <p>
      Importantly, we also find cases of the audible aggressive mimicry when a predator can imitate the sounds of another species well enough to trick them into making a dangerous mistake. One example, the Spotted Predatory Katydid, can imitate the distinctive violin sound of the female cicada, luring males close enough to become a tasty snack. One can speculate that this is a direct analogy to what might have happened to humans. Perhaps, something really big, with sharp teeth and pointy claws would hide in a cave, lying in wait while making noises that sounded very similar to the Proto-Indo-European for “help help”, casually eating off the members of Homo Sapiens. Unfortunately, members who did not have a strong aptitude for distinguishing real human voices from fake would get eaten, leaving alive those with an stronger in-born talent for avoiding mimicking predators. If so, perhaps we still carry this ability in our genes today, a remnant of our more precarious past, a talent that manifests itself when we find ourselves uncomfortably dealing with the Uncanny Valley.
    </p><br>
    <p>
      In an attempt to learn to mimic humankind’s linguistic ability, we’ve built computer algorithms to listen to thousand of billions of snippets of human language, all to teach itself using enormous computing power, how to talk like us. The results are the products (Siri, Alexa) we’ve heard about here. What we’ve learned, really, is that it’s very hard to mimic human speech to such a degree that actual humans don’t notice the difference. Very, very hard, apparently because our ears and brains have adapted incredibly well to recognizing what is and is not an actual human voice. This may be that instinct against mimicry learned long ago, and buried deep within our genetic heritage.
    </p><br>
    <br><h2>Linguistics in the Uncanny Valley</h2><br>
    <blockquote>
      &#34;My ex-wife still misses me. But, her aim is slowly improving&#34;
    </blockquote><br>
    <p>
      This joke, stupid at best, would be a nightmare for AI to figure out. It uses the two radically different definitions of a single word, “misses”, to create a situation where the reader is thinking about one meaning (longing for another person), only to realize that the other wildly different definition of of “shoot and miss with a gun” is actually intended.
    </p><br>
    <p>
      Human languages are all based on rules and patterns, yet many of these times the rules can be bent, broken, or ignored entirely while still sounding intelligible and, even grammatical. Human language all have a beautiful structural looseness to them, which allows us to speak flexibly, with many viable variations for a single phrase or word, and know that we will be understood. Computer programs, on the other hand, want inputs to be rigid and specific, the complete opposite. As a result, the in-built flexibility of human language (seen across all languages), causes a big problem for voice programs. In this section, we’ll examine those aspects of language itself which make it so difficult for AI to master.
    </p><br>
    <p>
      Here I will explain how the nature of language can be confusing to computers. First, we’ll look at the tangible, scientific components of language can leave a lot of uncertainty, which is a huge pain for computer programs. Second, we will look closely at body language and conversation flow, to see how these more ephemeral aspects of language can be an enormous nuisance for AI.
    </p><br>
    <br><h2>The Rules (of Language) Are Meant To Be Broken</h2><br>
    <p>
      The human brain’s linguistic ability has been evolving for millions, maybe tens of millions, of years, resulting in the fine-tuned linguistic machine that we all carry around in our heads. However, when it comes to our language software we struggle to accommodate some of the linguistic “tendencies” which humans and human languages all have which are particularly annoying for our programmers. The two biggest areas we see systemic anti-patterns in human languages are in the irregular rules and words that accumulate over time, and the flexibility found in certain words, phrases, or sentence patterns that allow them to be used correctly in lots of different contexts.
    </p><br>
    <p>
      Irregular rules almost always come from irregular patterns, which themselves are created organically by people who speak in ways that break the “traditional” grammar rules. Basically, people just speak however they speak, and if an improper usage spreads enough it becomes official. Once an irregular usage has become so widely-used that it feels like a part of the language, some governing body (such as the Oxford English dictionary) will declare it part of the language. A common example of this is changing word definitions, as a word such as “lousy” may start out as meaning “covered with lice”, but end up changing meanings when used by Geoffrey Chaucer to mean the more generalized “bad, poor”. Or, more contemporaneously, one could look at the word “meme” originated by Richard Dawkins in “The Selfish Gene”. Originally meaning “a cultural item transmitted via repetition and replication”, the internet turned into shorthand for any picture or video with images and text that was easily alterable via the individual, a definition already included today in most dictionaries.
    </p><br>
    <p>
      People usually create and speak with these irregular patterns for the most human of reasons - we’re lazy and it’s easier. We will shorten words, sentences, drop syllables or whole words, just to save our mouth muscles a little bit of extra strain. “Isn’t it” becomes “innit”, “abdominals” become “abs”, or “of the clock” becomes “o’clock”. Perhaps one of the most famous English slang words, “Y’all”, has one syllable less than “You all”, and thus has persisted in American South slang for hundreds of years based on this minor convenience[6].
    </p><br>
    <p>
      Sometimes, we create slang words just for fun; I’m convinced australians add “-o” to the end of a lot of words (“smoke-o” smoke break, “fisho”, fishmonger, “rego”, vehicle registration, to name just a few) just to spice up the language and make things more interesting, as in a few cases (smoke-o) they actually make the word longer. Or, the Parisienne slang Verlan of reversing the order of syllables in a word, so that “metro” becomes “tro-me” and “femme” becomes “meuf”. The phenomenon is named with an example of itself, it’s called “verlan” because that is the switched-syllable version of “l’enver”, itself meaning the english word “reversed”. Verlan often creates words with more syllables than the original, and so has other significance other than ease-of-use.
    </p><br>
    <p>
      Humans are constantly fine-tuning how we speak, which requires language software that can adapt to know what you’re talking about. If not, they are liable to get stuck clinging to old linguistic patterns and will fail when that language goes through natural adjustments. Powerful AI programs get to practice millions of times a day from real-world conversation snippets, to try to keep up with our language as it changes while still understanding all versions of that language throughout time. Thanks to big data, our machine learning algorithms actually get to practice most of these, and so are getting better by brute-force, teaching themselves all these rules and exceptions by example. But it’s slow going, so we’ll see in upcoming years if a program can really comprehend all the irregular and obscure patterns that human languages require for real “fluency”.
    </p><br>
    <br><h2>Say It However you want, You Are Probably Correct</h2><br>
    <p>
      Human language is amazing because it is so flexible. This makes it really hard for software, they don’t like flexible because it’s inherently complex and they need to be prepared for any possible input. But, it’s great for humans with our language equipment hardwired, since it allows us to be both more lazy and creative when we speak. We saw that humans don’t always follow the rules when speaking, and now let’s look at how human languages allow incredible flexibility within the language itself.  “Take out the trash, would you?” (especially said with a deep New Jersey accent) has a more menacing tone than “Would you take out the trash?” Simple word switches can massively alter the meaning of simple sentences, confounding computers algorithms, which are learning as fast as they can. But, it’s slow work.
    </p><br>
    <p>
      In the following paragraphs I will explain how there is a surprising amount of variety within individual languages, starting by examining how simple word choice offers a multitude of expression which can trip up software, then looking at grammar patterns that allow for interpretation and creative expression, and finally at how phrases and colloquial expressions can be an intentionally open-ended tool. I then want to explain why we humans have no problem dealing with this linguistic “looseness” (we come hard-wired with top of the line equipment), while software struggle immensely.
    </p><br>
    <br><h2>Pick a word, any word</h2><br>
    <p>
      Words can cause a lot of problems for software, all by themselves. The first reason is that most words have many other words that mean the same thing, or close to (synonyms), the second that a single word can have several (or dozens of) different meanings, all of which is confusing for a computer program which must be pre-programmed to handle all of this variation.
    </p><br>
    <p>
      To see how a language can pick up tens of thousands of synonyms, let’s look at a currently popular language called English. I use English in a lot of examples in this book as it is both my native language and one I find really fun to speak, but it has one very interesting fact that I have always loved. According to my Phonology Professor at the University of Barcelona, English has more nouns than any other language on the planet. Apparently, we Anglophones have a thing for specifics and categorization, and so we are happy to have seventeen words for “cup”, when fewer synonyms suffice in almost every other language.
    </p><br>
    <p>
      There are many interesting reasons for this, the first being that Modern English is the bastard love-child of German and French when William the Conquerer crashed his way into England in 1066. Everyone then started to simplify the verb tenses from Anglo-Norman (the pre-cursor to French), and the noun cases in German, while assimilating about ten or fifteen thousand French words into English, with a staggering number of vowels. Initially through the Church and stationed military in England that act as enforcement, but eventually through the Upper Class Aristocracy. This class structure can be even be seen in many of our duplicate words, just think of why we need a word for both “beef” (from French) and “steak” (German). This merging period lasted until they fully bled together to form “Middle English”. Middle English is also the closest a modern speaker might have any chance of speaking or understanding, if they were to be sent back in time. If a tardis lands in your backyard, don’t go back further than about 1300, you really won’t understand what anyone is saying, even in Britain.
    </p><br>
    <p>
      And this fact sucks for computers. Having lots of nouns is confusing for them, especially when things like age of the talker, slang, and regional upbringing can all affect the words you or I might choose. Hell, I change the words I might say depending how I feel in that exact moment, and the more synonyms a word has the more tempted I will be to do so. Sometimes I’ll even try out a new phrase I heard earlier in the day, or a word I learned on a podcast, without even thinking about it; I just naturally want to flex my linguistic muscles. Even a super smart AI like Alexa or Deep Blue might simply not have the contextual clues to figure out that “Alexa, find me a Pot place nearby” means I’m looking for kitchen utensils, as the algorithm is simply making an educated guess.
    </p><br>
    <br><h2>Linguistic Loose Coupling</h2><br>
    <p>
      Often times, the grammar rules of an individual language will offer a lot of flexibility, so a speaker can say the same sentence many different ways and have it mean the same, or almost the same, thing. This adds complexity for anything doing linguistic analysis, because it means that many different-looking sentences can actually mean the same thing. The phenomenon is incredibly common, in fact, let me show you what I mean being giving you a sampling of the various ways one can (correctly) write the English sentence “I went to the store yesterday, where I saw Todd”:
    </p><br>
    <blockquote>
      I went to the store, where I saw Todd, yesterday.<br>
	    Yesterday, I saw todd when I went store.<br>
	    Yesterday, I went to the store and saw Todd.<br>
	    I saw Todd, yesterday, when i went to the store.
    </blockquote><br>
    <p>
      And the list continues on. As you see, even basic sentences afford a lot of variety for the speaker, a phenomenon I call “Linguistic Loose Coupling”. Human languages are designed so the speaker can express their thoughts via language in pretty much any way they wish, and they will at least be understood. This is opposed to a traditional software-based approach, where inputs and outputs often need be perfectly formatted data for the software to run at all. If humans worked like this, we would only be able to understand perfect sentences, and we would spend all of our time obsessively checking our grammar before saying anything, because if we screwed up a single thing we would not be understood. It would be a nightmare.
    </p><br>
    <p>
      Instead, the data that flows out of our mouths and into our ears is interpreted “loosely”, where a person can describe a thought or feeling many different ways, using many different vocabularies. “Loose coupling” comes from computer science meaning the same thing - your data inputs do not need to be perfect for something to work. Tight coupling, the opposite, might be a system like your engine and gasoline - it needs a very specific type of fuel to be put into the tank, otherwise it won’t run, it will just seize up. Linguistic loose coupling is important for ease of communication, as our attention is split amongst an incredible amount of resources between our thought, our senses, and whatever else may be on our minds. Language needs to be easy to do so that when I order a bagel at 6:30am I can just mumble some nonsense and, if it’s close enough to sounding like “croissant and coffee”, I will almost certainly get just that.
    </p><br>
    <p>
      Sounds great! So what’s the problem? Well, for computers this adds a very, very significant amount of complexity, meaning that a machine has to learn, or at least be able to parse and interpret, all the various ways a user might express something. So, an easy command like “Alexa play the Mariah Carrey song ‘Butterfly’” when users can and will say it dozens of different ways without even realizing it.
    </p><br>
    <blockquote>
      &#34;Alexa, uhh, play that mariah Carey song about the wings of the butterfly, I think?&#34;
    </blockquote><br>
    <blockquote>
      &#34;Alexa, Butterfly, Mariah Carey.&#34;(Here Alexa needs to figure out which one is the song and which one the artist)
    </blockquote><br>
    <blockquote>
      &#34;Alexa please go ahead and play some Mariah Carey for us, how about a little ‘Butterfly’&#34;
    </blockquote><br>
    <blockquote>
      &#34;Alexa, play ‘Butterfly’ by Christina Aguil...Carey! Mariah Carey!&#34;
    </blockquote><br>
    <p>
      Any one of those sentences would have been more or less perfectly understood by any English speaker on Earth, and yet the variety is truly enormous. And this command, playing a song, is straight-forward compared to other fundamental tasks like contextual search that require even more complex linguistic analysis.
    </p><br>
    <p>
      Linguistic Loose Coupling also presents a big problem for machine learning software, the kinds like DeepMind or DeepBlue which are the brains behind some of the most advanced language software. These AI “train” themselves to improve at tasks, like language. They do this by “Guess and Check”, where they make millions and millions of guesses, and then check their work, tweaking their own algorithm afterwards to try and improve. The problem is, linguistic flexibility makes a “right” answer a debatable question, and so these machines have trouble creating proper guess-and-check scenarios for them to improve. Almost all linguistic answer are contextual, and so these AI are stuck when it comes to their training methodology which, again, is intended for highly specific inputs, not the loose language that we actually get.
    </p><br>
    <br><h2>Versatility</h2><br>
    <p>
      The way we humans use pre-set, repeatable phrases is very frustrating for software that is attempting perform linguistic parsing. An idiomatic phrase such as “Sounds like a plan!” (meaning “seems good”) can be extremely tricky for software as this phrase can be used in many different situations, with many different shades of mean. This kind of linguistic grey area is difficult for computers, added complexity and loose definitions always are, and so we find phrasing to be a key area of difficulty when a machine tries to understand human language. Let’s take a look at some examples of phrases that could prove difficult for software to see what makes these look so tricky.
    </p><br>
    <p>
      In Mandarin, there is a whole genre of idiomatic phrases called “chengyu” which consist of exactly four characters, are used in a wide variety of situations, and serve as philosophical answers to all classes of life issues. For an english speaker, a chengyu is similar to a phrase like “loose lips sink ships” or “have your cake and eat it, too”, they are generalized caches of knowledge, delivered as the situation requires. These four-character expressions are a highly valued part of Chinese culture, and one may be considered educated and intelligent for using them properly.
    </p><br>
    <p>
      To show you how this works, we can look at the example:
    </p><br>
    <blockquote>
      &#34;脚踏实地&#34;<br>
     (jiǎo tà shí dì)
    </blockquote><br>
    <p>
      literally meaning “To step on solid ground”. This phrase most often acts as advice, telling the listener to focus on the fundamentals. If I’m debating changing jobs or taking a class to get better at my current job, a friend might advise me to “jiǎo tà shí dì 脚踏实地 ”, meaning that I should work hard to improve my current situation. Or, I might feel overwhelmed by the amount of housework to do, a remind myself “jiǎo tà shí dì 脚踏实地” and start with the basic, most necessary cleaning first (in my case, this usually means doing dishes).
    </p><br>
    <p>
      As a software developer I often need to learn new technologies, and will often be tempted to jump right to the cool parts of this new tech first, but must tell myself that I better “jiǎo tà shí dì 脚踏实地 ” and read the documentation and familiarize myself with the very fundamentals. If not three things are apt to happen. First, I might misuse the software, exposing some kind of horrific vulnerability by not properly hitting every safety feature, which is in the docs. Second, I may not use the technology to it’s fullest potential by remaining ignorant of all it’s features, and third because it will be impossible to fix later.
    </p><br>
    <p>
      There are literally thousands of Chengyu, and they are used frequently in both written and spoken Mandarin, that can be used in any number of unpredictable patterns. And, unfortunately, the context (more on this later) of the situation and the relationship of the two speakers can drastically alter the meaning intended by a Chengyu speaker. This is, in fact, why Mandarin speakers consider a sign of high intelligence and conversational ability - when understood, they prove that both the speaker and listener are actively participating at all levels of the conversation. Not surprisingly, these are extremely hard for computers to understand or use.
    </p><br>
    <p>
      “Cool” is about as far a word from the chengyu, that bastion of linguistic sophistication, and yet they are exactly the same in this regard. In addition to its formal definition of “sort of cold”, the word “cool” has literally hundreds of meaning, most of them slang. Several are even direct contradictions of each other, such as:
    </p><br>
    <blockquote>
      Me: “Hey Billy would you like a copy of Zelda: Breath of the Wild?”<br>
      Billy: “Cool!” (yes)
    </blockquote><br>
    <p>
      This is a big contrast to something like:
    </p><br>
    <blockquote>
      Me: “Hey Billy would you like some of this lukewarm instant coffee?”<br>
      Billy: “Umm, it’s cool” (no thanks)
    </blockquote><br>
    <p>
      Cool is one of those slang words that can be used for almost anything, though it’s core meanings remain “popular”, “ok”, “not great”, “really great”, and “To be on merely okay terms with someone”. For many speakers, it even acts as a filler word, taking place of something like “umm” (“cool, so…”), effectively becoming meaningless linguistically as anything more than just noise. For any non-native speakers coming to America I recommend this as a core word to pick up, since it will make you sound like a more native speaker while, when properly used, also allowing you these nice long linguistic pauses to allow you to prepare your next sentence.
    </p><br>
    <p>
      Every language contains these multi-use elements, the ones a speaker can say in a million different situations. As we’ll see, our ability to handle context-base phrase diversity is one of the factors that put us miles ahead of language software.
    </p><br>
    <br><h2>Nonverbal Communication </h2><br>
    <p>
      We’ve just seen how computers have a difficult time with language because there are so many rules that we like to break, but now let’s look at parts of language that have no rules at all, and are even more difficult for software to manage. Let’s take a look at some of the more ephemeral aspects of language which really gives computer programs a difficult time. First, we’ll look at nonverbal communication such as body language and facial gestures, to demonstrate that a lot of communication happens in addition to actual words, then we’ll see how tone of voice plays a huge role in understanding speaker context while talking, and finally we’ll dig into the general conversation, and see how “flow” and interconnectedness plays a huge role in our day to day communications.
    </p><br>
    <p>
      Non-verbal communication is a funny thing, as it doesn’t feel all that important until you miss a nonverbal cue in a conversation (a knowing wink, perhaps) and completely misunderstand what was said. The more we study humans and how we speak, the more research has appeared on the importance of nonverbal gestures in language, such as eye movement, shoulder shrugs, pursed lips, head nods or any other way we humans share information without explicitly stating it. In fact, the effect is so strong that experts recommend against using sarcasm in written communications like text or email, since these require a signal to the reader that the message is intended to be sarcastic. Otherwise, they just read your message in there head with their normal voice, and won’t realize that it is not a joke. One study claims that up to 55% of our conversations are actually done via non-linguistic methods [7], though I honestly have no idea how one might prove that. I want to take this opportunity to explain why it is so uncommon for language software to take non-verbal communication into account, and why it would be difficult for them even if they did.
    </p><br>
    <p>
      While it’s easy to see why non-verbal communication is such an important part of language (just shake your head “no” if you don’t), it’s also clear that the technical engineering needed to add this to current voice software is extremely challenging. New hardware will need to be tied into even monitor someone’s bio-markers to even properly read their facial expressions, body language, or stress signs like heartbeat or sweating. Technology to perform these functions does exist, but they are quite different from the machines we use for voice, so a smooth integration of bio-reading technology into voice systems will need to occur on a product level. For some non-verbal communication methods this won’t be an issue because they only require a camera (facial expressions, most notably). For others, especially technologies that may monitor vital signs like heartbeat, this could be a long way off.
    </p><br>
    <p>
      This is also very invasive, as many people are concerned with privacy and data security, and the idea of being monitored to this degree can seem creepy. Some devices, like a Kinect, Apple Watch, or Fitbit already does record some of this data, but these are for explicit uses like fitness, and I wouldn’t be optimistic about consumers dying for voice software that requires monitoring to this degree. Consumers view monitoring suspiciously at best, and at worst feel like their closest secrets are being spied upon.
    </p><br>
    <p>
      It’s a moot point at the moment, however, as they don’t even have the data for an AI to incorporate non-verbal communication into its algorithms. We might want to allow Siri to measure our facial expressions or heartbeat to better detect sarcasm (or maybe not, honestly), but that would require a lot of very specific data. A product like Fitbit or Kinect may use its camera to collect lots of biometric data like heartbeats, and a product like Siri collects lots of voice data to help get better at talking to you, but as far as I know there are not many (or any) devices which collect biometrics and use them to improve their voice technology. Until these two can be collected and analyzed as a group it will be very difficult to train an AI to use information from the body and face of the speaker to improve voice algorithms, because they machine will be unable to tie the two together.
    </p><br>
    <br><h2>Tone Of Voice</h2><br>
    <p>
      The tone of voice that someone uses while speaking can say a whole lot about what they are saying, and how they feel about it. Even a seemingly happy phrase like “Hello, pal.” can start to sound ominous or threatening when used angrily, in a falling tone where the last word “pal” is almost spat out. Tone of voice is, generally, the act of raising and lowering your voice during parts of speech to add characteristics to parts of what you are saying. Tone, like body language, is a difficult part of the language equation for software because there is so much personalization and ambiguity around what tonal patterns might mean, depending on a lot of external factors regarding the speaker which the program might not know. Linguistic tonal patterns can be regional, cultural, or widely unique to the person, and so software needs the ability to manage all of these variations. In this section, I would like to start with an example, High Rising Tone, to clarify exactly why tone is so tricky. I would also like to dive into the difficulties that machine learning techniques face with tone of voice, and finally how it might one day overcome these hurdles. I would like to clarify that I’m speaking only of “tone of voice” in non-tonal languages such as French or Portuguese. In a tonal language (e.g. Mandarin ot Thai), tone is literally used as semantic part of speech; it literally changes the meaning of words, where are defined by a tone as well as the sounds made.
    </p><br>
    <p>
      Raising your voice at the end of a sentence may be one of the trickiest examples of tone of voice. You raise your voice at the end of a sentence to make a question, right? But what if you’re Australian or, especially, from New Zealander, then you might raise your voice at the end of a normal sentence as part of your speech pattern. If you’re from the San Fernando Valley, or parts of the Pacific Northwest, then you might also end a lot of sentences that way, all day, everyday. It’s just part of your accent. But how is a computer supposed to know this, especially if you are a relatively new user?
    </p><br>
    <p>
      This inflection, called a High Rising Terminal, is a much more generalized part of speech (more like a sound, such as “thhh” or “ooo”) then many English speakers might think. Many, many languages do not use an HRT inflection to make a sentence a question. In fact, tonal languages literally cannot change the tone or inflection of their speech like this, at all. In Mandarin, for example, It would completely change the meaning of the words. Even the basic “Nin hao ma” (how are you?) could be interpreted as “You’re good, mammoth” if the last syllable is given that high-rising lilt at the end, and heard as a fourth tone. And, in fact, English could survive very well without the HRT as well. After all, with no punctuation or HRT you could still easily discern most questions from statements, as I’m willing to bet that, even without punctuation, you know which of these two is supposed to be a question:
    </p><br>
    <blockquote>
      “We are going to the store to buy ice cream”
    </blockquote><br>
    <p>
      And,
    </p><br>
    <blockquote>
      “Are we going to the store to buy ice cream”
    </blockquote><br>
    <p>
      I argue that that, without the inflection (or the punctuation, even), we’d all pretty much understand what is and is not a question through word choice alone. We keep the High Rising Terminal because we like it, it’s kind of fun, and until we stop using this part of speech it will remain a confounding vex for our computers when we do voice. This is an example of a nuance that nature has prepared us humans to be very good at picking out, but which is hard to build software to manage. After all, does the software engineer write a true/false statement every time Alexa hears that uptick to ask “Ok, is he from New Zealand? What about Studio City?” It’s not impossible, a subprogram like this could be written. But it’s not easy, and getting this right with someone the first time your computer ever talks to a user is pretty much an educated guess.
    </p><br>
    <p>
      Tone of Voice presents lots of variance problems like the one above. As mentioned, it’s very easy to miss a sarcastic tone, especially for speakers with a different background than you. Angry tones are difficult, too, as these are even easily misinterpreted by humans. As an example, almost any language in the world when heard on the street, spoken quickly, when heard by non-speakers will sound angry. The languages I’ve personally heard characterized this way include Spanish, Mandarin, German, Russian, Ukrainian, Turkish, Arabic, and undoubtedly many more (though never Brazilian Portuguese, hmm…). And, I can tell you that these are all beautiful, poetic language at every speed, however our ear simply thinks it sounds fast (since we don’t know what they are saying), and this gets mangled up and translated emotionally as “angry”.
    </p><br>
    <h4>LISA GETTING DIRECTIONS IN RUSSIAN</h4>
    <p>
      This variance is what trips up the language software, when it comes to voice. For a machine to take into account a speaker's tone of voice, they would very likely need to collect and analyze speech data from that user before they could get started. Unfortunately for AI, linguistic tonal patterns do not seem to generalize very well to the general population. Analyzing tonal patterns of every single user on your voice product, such as an Alexa, is not impossible but it does take a lot of computing power. It will also take time, as the data actually needs to be collected from the speaker before machine learning programs can be run to incorporate this tonal knowledge into its algorithm. It will require nuanced analysis programs and sensitive equipment to master tone of voice as a part of linguistics.
    </p><br>
    <br><h2>The Dao of Conversation</h2><br>
    <p>
      Conversation just makes us happy. We all have that one story about a terrible day when everything was bad, and you talked to a stranger on the bus or an old friend on the phone, and suddenly you felt much, much better. Often times, you didn’t even talk about your problem at all, just the act of conversing put you in a better mood. What is conversation, and why does it have such a strong effect on us?
    </p><br>
    <p>
      Interestingly, and I think ironically, the word “conversation” aggressively fights a standard definition. It is one of those words that mean something slightly different for everyone, amazingly so since the idea of conversation (as opposed to just speaking) is one of the most fundamental aspects of language. Some experts feel that conversation has an informational purpose, and define the word to mean “interaction of ideas”. Others argue that “etiquette” and “bolstering social ties” are the most important parts, and see the exchange of information as secondary. We will look at the “Rules of Conversation” (according to the cooperative principle), and see how their vague nature actually makes it easier to see clearly based on what is “not conversation”, which I will demonstrate through a few examples. We will then try to clarify what exactly the word really means, boiling down the essences from different perspectives on the word.This will let us see why it’s ephemeral, shifting nature means that language software has a hard time with the subject, and as such is a huge culprit for the verbal uncanny valley effect. We’ll take a look at how this occurs, and what the future for advancements in “conversational Artificial Intelligence” actually looks like.
    </p><br>
    <p>
      A good conversation is like a two-person juggling act. In both cases, each participant must pay close attention to what the other is doing so that each can smoothly and expertly react to the others’ actions, all for the purpose of keeping an ongoing, sustained event. And failure is defined the same - the event ends; so when the juggling balls start hitting the floor or, in the case of conversation, an awkward silence ensues, you know it’s over. While many, many people have tried to impose top-down rules for what can be part of a conversation (my mom’s favorite: “no politics”), the only immutable rule is “Don’t do anything that ends the conversation”, except of course to dismiss yourself politely. Not the most helpful rule, as a speaker only finds out they broke it after the fact by seeing other people in the room start to look at their watches while moving towards the door. But, we can use this rule as a starting point to look at some counterexamples of what is not actual conversation.
    </p><br>
    <p>
      Topics that are generally not considered conversational are usually those which are either controversial or off-putting. While this may seem annoying to some of us, these topics lack conversationality because they are likely to cause conversation participants to either dismiss themselves, or potentially create a general air of animosity that itself will end the conversation. Avoiding these conversation-ending events requires a strong ability to “read the room”, and determine which topics the others around you might find agreeable and interesting and, most importantly, not awkward. A master of this ability might have a preternatural ability to read context, and intuit what areas of conversation might hold the most interest for those around the room based on whatever clues can be gathered. Language software has a hard time with this because it has very little ability to guess the mindset, politics, and history of each speaker it interacts with. Even though it can probably cull some of this information from social media accounts or previous history interacting with you, machine learning software is notoriously bad at predicting how you may feel, or in what areas you might be especially sensitive.
    </p><br>
    <p>
      Highly structured or focused talking is also not included under the banner of conversation. Apparently, we require a level of informality and spontaneity in our conversations, and so anything written ahead of time doesn’t count, whether it’s belaying a rock climber or going through the rituals to meet the emperor of Japan. This is also why interactions where rank plays a significant part are not considered true conversation; whether with a boss or a military superior, this removes the element of spontaneity. In general, tightly focused topics are not considered conversations either, as they are not spontaneous but rather entirely for a single issue. Though this definition is debated heavily especially by technical people who participate in spirited, raucous conversations about highly specific topics (Linux vs. Unix anyone?) all the time. In fact, I’m pretty sure that “conversations about highly specific topics” is the base of all nerd cultures. But, these are free-flowing open conversations as opposed to, say, a meeting of four-star general discussing tactical responses to a threat.
    </p><br>
    <p>
      So, conversation seems loose and spontaneous compared to other, more structured forms of speaking. And, to keep a conversation aloft, the conversationalists need to have an understanding of the attitudes and mood of their speaking partners so as not to endanger prematurely ending the conversation. Both of these skills are very much of an “intuitive” nature for human beings, which is a bad sign of our software as it requires the ability to easily adapt to the situation at hand, rather than relying on pre-written instructions.
    </p><br>
    <p>
      Unfortunately, that means that when AI tries to be conversational, it can often become a literal “conversation killer” by ungracefully bringing up taboo subjects (such as Microsoft’s chatbot Tay which became racist [8]), and almost always sticks out as being fundamentally unable to gracefully bounce off of other speaker’s statements to keep a conversation going.
    </p><br>
    <p>
      It’s unknown at this time if Artificially Intelligent algorithms that use Machine Learning to improve their language skills could ever flawlessly mimic a human being. I’m not so sure, personally. I mean, who knows? Maybe if you feed millions and millions of conversations and speeches and accents into a big huge mainframe, it can learn to mimic a human. But maybe there’s some ephemeral je-ne-sais-quoi to language that a computer could never really perfectly imitate. For the same reason that an artificial flavor could never really taste exactly like a fresh picked strawberry, I don’t know that a computer could truly pull of speaking in the voice of, say, my East Coast Grandmother, without me knowing the difference. In both cases, almost anyone could tell the difference every single time.
    </p><br>
    <p>
      If the expectation for conversation level is set to, say, a C3PO-level assistant, that I’m all in for in the next 20 years or so, maybe less. Honestly,C3PO almost seems like the Amazon’s Echo’s British boarding school cousin, who is a few years older and wiser, though he was never famous for being intelligent (that was R2D2). Our Machines are pretty good, too bad they have such a hard time holding the thread of a conversation. Perfection will be very difficult for Machine learning in this field, but ML does have the possibility of helping us make some incredible advances in the field, and in fact already has. But even basic conversational skills has been elusive in artificial intelligence, so there are no guarantees how fast or slow this part of voice technology may advance in our lifetimes.
    </p><br>
    <br><h2>Our Flexible Brains</h2><br>
    <p>
      In this section I’ve hammered over and over how it is the vague, ephemeral parts of language which give our software the most issues. But, this begs the question of why we, human beings, can handle this stuff no problem. Shouldn’t we have the same problems as the computers when it comes to vague parts of speech that require a ton of context to understand? The answer is “Yes, but we have really, really great language hardware in your brain”. Coming up we’ll take a look at the biology of speech, but first I wanted to demonstrate some examples of how adaptable and amazing your brain really is, and see why AI software will have such a hard time replicating our natural abilities to the extent that it no longer seems part of the uncanny valley.
    </p><br>
    <p>
      First, it is simply astounding how good human beings are at learning languages. In my own personal experience, I started taking a neurobiology course taught in Catalan while at the University of Barcelona. I could speak Spanish, a closely related language, but not Catalan itself, and was amazed to discover within 2 weeks of class long, 3 hour lectures I could understand almost everything the professor said. My brain just figured it out - the grammar, the syntax changes, almost everything. The CIA understands this, since they have mastered the art of teaching adults to become fluent in new languages in (rumored) 12 - 18 months. They take advantage of our natural linguistic sponginess, using immersion as an important tool (like I did in Barcelona) to train nearly flawless speakers. Evolution did not want us to do when we washed up in some strange region of the world with no way home, it wanted us to adapt to the surrounding cultures and made sure our brains have the ability to do just that.
    </p><br>
    <p>
      Like many language nerds, I love to travel, especially to places with interesting aspects to the language. My first time in Scotland I was delighted by how little I could understand of what was ostensibly my native language, and on the first day I was given a sympathetic overview of the local vocabulary of modern Edinburgh slang by a used-record store clerk who was very, very hungover, giving me some tips on Scottish slang (the only one I remember now is “bae” means “baby”, one that I think has arrived recently to the USA). On this first day I could barely understand a word being said by anybody, but by the third day I understood almost everything said to me by even the most drunken of Scottish revellers. My brain just naturally adapted to the accent, while I physically did nothing more strenuous than walk around the Edinburgh Festival drinking beer and watching street performers.
    </p><br>
    <p>
      We can also understand non-native speakers of our languages quite well, even when they are far from perfect speakers. Some my most interesting conversations were when either I or the other speaker was speaking in a non-native language. In fact, I spoke Spanish exclusively with a Swedish friend while living in Madrid, and we had long, philosophical conversations about everything imaginable late into the Madrileno night, both of a speaking in a foreign language.  And I marvel at how often I’m thrust into incoherent, tedious conversations in my own native language with other native speakers, demonstrating that it’s the content, not the language ability, that makes a conversation interesting.
    </p><br>
    <p>
      We adapt linguistically to our surrounding very well, we can both learn regionalisms on the fly and whole new languages if we need to, and our brains our able to handle imperfect speech patterns much better than software currently. In the next section, I want to take a look at our biological knowledge of speech, and why we can’t use this to create more effective language software.
    </p><br>
    <br><h2>Will Siri Cross the Uncanny Valley?</h2><br>
    <p>
      Siri may be mired in the Uncanny Valley right now, but that could change. Business and programs worldwide are working on some revolutionary technologies which could solve many of the problems discussed here. Let’s look closeup at a few products and how they might help make this happen.
    </p><br>
    <p>
      I personally find “conversation” to be one of the most interesting challenges posed so far. How do you program the ability to be a good conversation partner? We can barely define the word “conversation”, let alone teach a computer program to be conversational. Yet, there are several “Therapy Apps” new to the market (TalkSpace is the current leader in the app store), with several more coming soon, that I think could do just that. Until now, most robotic conversation partners speak with an explicit purpose, usually to give information or perform a task. A Therapy app, however, has conversation itself as its purpose.
    </p><br>
    <p>
      An important part of therapy is to get the patient to feel comfortable enough to talk openly about themselves (for example, nothing in Freudian therapy can be forced, otherwise it is not accepted). A therapy app, therefore, would have the purpose of keeping an easy, natural conversation going with the user, and the product developers for a successful version of this app would need to overcome many of the challenges described earlier when we talked about conversation. I don’t expect these apps to solve all of those problems, they are some of the toughest issues in linguistics, but I could imagine a therapy app startup making huge advances in this niche.
    </p><br>
    <p>
      It’s video game companies, fitness, and Internet of Things products, however, which will likely make the greatest advances in varied data collection, another prerequisite to creating truly post-Turing language software. As discussed, we will likely need to collect a multitude of data from the user to create software that can perfectly understand them. Body language, tone of voice, and other factors make these necessary to truly great voice analysis. And, it is gaming platforms like Kinect and Playstation 4 which use voice for connected playing, where online teams can go on missions in Call of Duty or friends can go head to head in Rocket league, all of uses microphones during complicated interactions. Fitness products like Fitbit collect a lot of biometric data such as heartbeat, steps taken, and calories burned, and other similar IOT gear is coming out that can measure insulin and blood sugar levels. If these companies can begin to use this data in a big way with voice products they may be working on, they could add the crucial context that software currently lacks in many situations. This is the type of data that can help determine the mood and attitude of a user, and so help decipher the inner meaning of whatever they might say (I’m thinking of myself yelling angrily at my Nintendo). This could allow one of these companies to create in-game voice interactivity that was next-level, as well as cracking some of the problems talked about here.
    </p><br>
    <p>
      Of course, major voice products like Alexa and Siri will hopefully be some of the biggest drivers of progress, especially at using Big Data analysis to train AI capable of exiting the far side of the Uncanny Valley. These companies collect and process truly insane amounts of data, and a huge benefit they will provide the market is by analyzing this data to create amazing artificial intelligence. With this data, collected from literally billions of users, companies like Amazon, Apple, and Google have the ability to train language AI better than anyone else.
    </p><br>
    <p>
      These companies obviously have a huge incentive, Amazon, for example, has one of the best reasons to get it’s Echo home-assistant in as many places as possible - it creates more sales through Amazon itself. A report [9] that came out 6 months ago states an average 6% increase in sales by Amazon Echo owners, something which Jeff Bezos and company should be remarkably excited about as with enough customers that could easily translate to billions of dollars. This is even if they don’t make any money on the Echo devices themselves, which is certainly the reason they’re aggressively pushing the $49.99 Echo Dot, even thought at that price it is almost definitely not itself a money-maker. For them, it’s just another distribution platform to sell product.
    </p><br>
    <p>
      Finally, Universities, Health research, and other organizations are consistently pushing the boundaries of what we know about how language actually works physically in the brain. It may be surprising, but even our best and brightest struggle to understand the actual nature of language. This might seem confusing, after all you probably think you know how language works. It feels so simple, if I say “I want a fish taco”, everyone in hearing distance knows that, if at all possible, you would like to get your hands on a fish taco, intended for direct consumption.
    </p><br>
    <p>
      But what does your brain actually do when you hear these words? Did the words flash before your eyes, with a brief definition of each highlighted below, forming a nice easy sentence for you to read? Or did you picture, smell, or even taste the last taco you ate, maintaining that feeling for 1/1000 of a second? Probably neither, but I bet your linguistic experience was closer to the second than the first, and there was probably a little bit of both involved. We have very little insight into how these processes work in the brain - in fact we’re not even sure exactly where they take place, or in what order. Our lack of knowledge about how language works in the brain has been a hindrance; it’s prevented us from using the one linguistic model we know works (our own). It’s very hard to learn about a live brain (also very dangerous), and autopsied brains offer very little information about how the deceased spoke, or any linguistic behaviour.
    </p><br>
    <p>
      However, modern technologies such as MRI, laser scanners, and others are allowing both private and public organizations to make big advances in analyzing and using the human brain to advance voice technology. In fact, Facebook recently demonstrated a prototype for a user to type words on a screen using only their minds. These breakthroughs at studying our actual biological linguistic processes could provide models we can cull on to create next-generation voice, as well as teach us more about how we speak to each other as human beings.
    </p><br>
    <mbp:pagebreak />
    <h2>Sources</h2><br>
    <ul>
      <li>
        [1] https://www.csee.umbc.edu/courses/471/papers/turing.pdf
      </li>
      <li>
        [2] http://www.imdb.com/title/tt2084970/
      </li>
      <li>
        [3] https://www.techdirt.com/articles/20140609/07284327524/no-supercomputer-did-not-pass-turing-test-first-time-everyone-should-know-better.shtml
      </li>
      <li>
        [4] http://www.retaildive.com/news/npd-echo-device-owners-spend-more-on-amazon/426505/
      </li>
      <li>
        [5] https://www.amazon.com/Blink-Power-Thinking-Without/dp/0316010669
      </li>
      <li>
        [6] Though, “y’all” actually does cover a linguistic function not seen in English - the one word 2nd person plural pronoun, for which Spanish uses ustedes and french vous.
      </li>
      <li>
        [7] https://www.psychologytoday.com/blog/beyond-words/201109/is-nonverbal-communication-numbers-game
      </li>
      <li>
        [8] http://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist
      </li>
      <li>
        [9] http://www.businessinsider.com/amazon-echo-owners-are-spending-more-money-on-amazon-2016-9
      </li>
    </ul>
  </body>
</html>
